import { NextRequest, NextResponse } from 'next/server'
import { getServerSession } from 'next-auth'
import { authOptions } from '@/lib/auth'
import { prisma } from '@/lib/prisma'
import { VideoModel } from '@prisma/client'
import { analyzeUserRequest } from '@/lib/ai-orchestrator'
import { validateAndMapVideoDuration } from '@/lib/duration-utils'
import { WavespeedService, WanService } from '@/lib/ai-services'
import { GenerationType } from '@/types'
import { getSafeGenerationType } from '@/lib/generation-utils'
import { checkAndDeductCreditsForGeneration, refundCredits } from '@/lib/credit-manager'
import { z } from 'zod'

const chatRequestSchema = z.object({
  message: z.string().min(1, 'Message is required'),
  images: z.array(z.string()).optional(),
  audio: z.array(z.string()).optional(),
  chatId: z.string().optional().nullable(),
})

const upscaleRequestSchema = z.object({
  action: z.literal('upscale'),
  imageUrl: z.string().min(1, 'Image URL is required'),
  chatId: z.string().optional().nullable(),
})

async function handleUpscaleRequest(session: any, imageUrl: string, chatId?: string | null) {
  try {
    console.log('ðŸ” Starting upscale process for:', imageUrl) // Debug log
    console.log('ðŸ” Session user ID:', session.user.id) // Debug log
    console.log('ðŸ” Chat ID:', chatId) // Debug log
    
    // Check and deduct credits
    const upscaleCredits = await checkAndDeductCreditsForGeneration(
      session.user.id,
      'IMAGE_UPSCALE'
    )
    
    if (!upscaleCredits.success) {
      return NextResponse.json(
        { 
          error: `You don't have enough credits to upscale images. You need ${upscaleCredits.cost} credits. Please upgrade your plan to continue.`,
          required: upscaleCredits.cost,
        },
        { status: 402 }
      )
    }
    
    console.log(`ðŸ’³ Credits deducted for upscale: ${upscaleCredits.cost}`)
    
    // Create or get chat
    let chat
    if (chatId) {
      chat = await prisma.chat.findFirst({
        where: {
          id: chatId,
          userId: session.user.id,
        },
      })
      console.log('ðŸ” Found existing chat:', chat?.id) // Debug log
    }
    
    if (!chat) {
      console.log('ðŸ” Creating new chat') // Debug log
      chat = await prisma.chat.create({
        data: {
          userId: session.user.id,
          title: 'Image Upscaling',
        },
      })
      console.log('ðŸ” Created new chat:', chat.id) // Debug log
    }

    // Create assistant message for the upscale operation
    console.log('ðŸ” Creating assistant message') // Debug log
    const assistantMessage = await prisma.message.create({
      data: {
        chatId: chat.id,
        role: 'ASSISTANT',
        content: `I'm upscaling your image to higher resolution. This will take a few moments... (${upscaleCredits.cost} credits used)`,
      },
    })
    console.log('ðŸ” Created assistant message:', assistantMessage.id) // Debug log

    // Start the upscaling process
    console.log('ðŸ” Calling WavespeedService.upscaleImage') // Debug log
    const requestId = await WavespeedService.upscaleImage(imageUrl)
    console.log('ðŸ” Got request ID:', requestId) // Debug log
    
    // Use safe generation type to avoid database enum issues
    const generation = await prisma.generation.create({
      data: {
        messageId: assistantMessage.id,
        type: getSafeGenerationType('IMAGE_UPSCALE', 'TEXT_TO_IMAGE') as any,
        status: 'PROCESSING',
        prompt: 'Image upscaling to 4K resolution',
        provider: 'wavespeed',
        model: 'image-upscaler',
        requestId,
        metadata: { creditsUsed: upscaleCredits.cost }
      },
    })
    
    console.log('ðŸ” Created upscaling generation:', generation.id, 'RequestId:', requestId)

    // Get the updated assistant message with generations
    const updatedAssistantMessage = await prisma.message.findUnique({
      where: { id: assistantMessage.id },
      include: { generations: true },
    })

    // Get the user message ID from the previous message in the chat
    const userMessage = await prisma.message.findFirst({
      where: {
        chatId: chat.id,
        role: 'USER',
      },
      orderBy: {
        createdAt: 'desc',
      },
    })
    
    return NextResponse.json({
      response: 'I\'m upscaling your image to higher resolution. This will take a few moments...',
      chatId: chat.id,
      messageId: assistantMessage.id,
      userMessageId: userMessage?.id,
      generations: updatedAssistantMessage?.generations || [],
      isUpscaling: true, // Flag to indicate this is an upscaling operation
    })
  } catch (error: any) {
    console.error('âŒ Upscale error:', error)
    console.error('âŒ Error stack:', error?.stack)
    console.error('âŒ Error details:', {
      message: error?.message,
      name: error?.name,
      code: error?.code,
      response: error?.response?.data
    })
    return NextResponse.json(
      { 
        error: 'Failed to start image upscaling', 
        details: error?.message,
        stack: process.env.NODE_ENV === 'development' ? error?.stack : undefined
      },
      { status: 500 }
    )
  }
}

export async function POST(request: NextRequest) {
  console.log('ðŸŒŸ === CHAT API STARTED ===') // Debug log
  try {
    console.log('ðŸ“¨ Chat API called') // Debug log
    
    const session = await getServerSession(authOptions)
    console.log('ðŸ‘¤ Session check:', session ? 'Valid session found' : 'No session') // Debug log
    
    if (!session?.user?.id) {
      console.log('âŒ No valid session found') // Debug log
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      )
    }

    const body = await request.json()
    console.log('ðŸ“‹ Raw request body:', body) // Debug log
    
    // Check if this is an upscale request
    if (body.action === 'upscale') {
      const { imageUrl, chatId } = upscaleRequestSchema.parse(body)
      console.log('ðŸ” Processing upscale request:', { imageUrl, chatId }) // Debug log
      return await handleUpscaleRequest(session, imageUrl, chatId ?? null)
    }
    
    // Handle regular chat request
    const { message, images = [], audio = [], chatId } = chatRequestSchema.parse(body)
    
    console.log('ðŸ“ Parsed request:', { message, images: images.length, audio: audio.length, chatId }) // Debug log

    // Get user preferences
    let userPreferences
    try {
      userPreferences = await prisma.userPreferences.findUnique({
        where: { userId: session.user.id }
      })
    } catch (dbError) {
      console.error('âŒ Database error when fetching preferences in Chat API:', dbError)
      
      // If there's an enum value error, use default preferences
      if (dbError instanceof Error && dbError.message.includes('not found in enum')) {
        console.log('ðŸ“ Enum value error in Chat API, using default preferences')
        userPreferences = {
          id: 'default',
          userId: session.user.id,
          aspectRatio: 'SQUARE' as any,
          textToImageModel: 'SEEDREAM_V4' as any,
          imageToImageModel: 'SEEDREAM_V4_EDIT' as any,
          videoModel: VideoModel.WAN_2_5,
          createdAt: new Date(),
          updatedAt: new Date()
        }
      } else {
        throw dbError
      }
    }

    // Create default preferences if not found
    if (!userPreferences) {
      // First verify the user exists
      const user = await prisma.user.findUnique({
        where: { id: session.user.id }
      })
      
      if (!user) {
        return NextResponse.json(
          { error: 'User not found' },
          { status: 404 }
        )
      }
      
      try {
        userPreferences = await prisma.userPreferences.create({
          data: {
            userId: session.user.id,
            aspectRatio: 'PORTRAIT',
            textToImageModel: 'SEEDREAM_V4',
            imageToImageModel: 'SEEDREAM_V4_EDIT',
            videoModel: VideoModel.WAN_2_5,
          }
        })
      } catch (createError) {
        console.error('âŒ Failed to create preferences in Chat API:', createError)
        
        // If creation fails due to enum issues, use default preferences
        if (createError instanceof Error && createError.message.includes('not found in enum')) {
          console.log('ðŸ“ Enum value error during creation in Chat API, using default preferences')
          userPreferences = {
            id: 'default',
            userId: session.user.id,
            aspectRatio: 'PORTRAIT' as any,
            textToImageModel: 'SEEDREAM_V4' as any,
            imageToImageModel: 'SEEDREAM_V4_EDIT' as any,
            videoModel: VideoModel.WAN_2_5,
            createdAt: new Date(),
            updatedAt: new Date()
          }
        } else {
          throw createError
        }
      }
    }

    console.log('ðŸ› ï¸ User preferences:', userPreferences) // Debug log

    // Get conversation context for intelligent image referencing
    let conversationContext
    if (chatId) {
      const recentMessages = await prisma.message.findMany({
        where: {
          chat: {
            id: chatId,
            userId: session.user.id,
          },
        },
        include: {
          generations: {
            where: {
              status: 'COMPLETED',
              type: {
                in: ['TEXT_TO_IMAGE', 'IMAGE_TO_IMAGE']
              }
            },
            orderBy: {
              createdAt: 'desc'
            },
            take: 1
          }
        },
        orderBy: {
          createdAt: 'desc'
        },
        take: 5
      })

      // Find the most recent completed image generation
      const recentImageGeneration = recentMessages
        .flatMap((msg: any) => msg.generations)
        .find((gen: any) => gen.resultUrls && gen.resultUrls.length > 0)

      conversationContext = {
        recentMessages: recentMessages.map((msg: any) => ({
          role: msg.role,
          content: msg.content,
          hasGenerations: msg.generations.length > 0,
          generationTypes: msg.generations.map((g: any) => g.type)
        })),
        hasRecentImageGeneration: !!recentImageGeneration,
        recentImageUrl: recentImageGeneration?.resultUrls?.[0]
      }
    }

    // Analyze the user request with context
    console.log('ðŸ”„ Calling analyzeUserRequest...') // Debug log
    const analysis = await analyzeUserRequest(message, images.length > 0, audio.length > 0, conversationContext)
    
    // Initialize voiceId for potential use in response
    let voiceId = null
    
    console.log('ðŸŽ¯ Analysis result:', analysis) // Debug log
    console.log('ðŸŽ¯ Analysis action:', analysis.action) // Debug log
    
    // Create or get chat
    let chat
    if (chatId) {
      chat = await prisma.chat.findFirst({
        where: {
          id: chatId,
          userId: session.user.id,
        },
      })
    }
    
    if (!chat) {
      chat = await prisma.chat.create({
        data: {
          userId: session.user.id,
          title: message.slice(0, 50) + (message.length > 50 ? '...' : ''),
        },
      })
    }

    // Create user message
    const userMessage = await prisma.message.create({
      data: {
        chatId: chat.id,
        role: 'USER',
        content: message,
        images: images,
      },
    })

    let response = ''
    const generations = []

    // Process based on analysis
    console.log('ðŸš€ Processing action:', analysis.action) // Debug log
    switch (analysis.action) {
      case 'text_to_image':
        try {
          console.log('Generating image with prompt:', analysis.prompt) // Debug log
          
          // Check and deduct credits
          const imageCredits = await checkAndDeductCreditsForGeneration(
            session.user.id,
            'TEXT_TO_IMAGE'
          )
          
          if (!imageCredits.success) {
            response = `Sorry, you don't have enough credits to generate an image. You need ${imageCredits.cost} credits but only have ${await prisma.user.findUnique({ where: { id: session.user.id }, select: { credits: true } }).then(u => u?.credits || 0)} credits remaining. Please upgrade your plan to continue.`
            break
          }
          
          console.log(`ðŸ’³ Credits deducted: ${imageCredits.cost}`)
          
          const requestId = await WavespeedService.textToImage(
            analysis.prompt,
            userPreferences.textToImageModel,
            userPreferences.aspectRatio
          )
          
          const generation = await prisma.generation.create({
            data: {
              messageId: userMessage.id,
              type: 'TEXT_TO_IMAGE',
              status: 'PROCESSING',
              prompt: analysis.prompt,
              provider: 'wavespeed',
              model: `${userPreferences.textToImageModel.toLowerCase()}`,
              requestId,
              metadata: { creditsUsed: imageCredits.cost }
            },
          })
          
          generations.push(generation)
          response = `I'm creating an image based on your description: "${analysis.prompt}". This will take a few moments... (${imageCredits.cost} credits used)`
        } catch (error: any) {
          console.error('Text-to-image generation error:', error)
          console.error('Error details:', error.response?.data) // More detailed error logging
          response = 'Sorry, I encountered an error while trying to create your image. Please try again.'
        }
        break

      case 'image_to_image':
        // Handle both uploaded images and recent image references
        const imagesToUse = images.length > 0 
          ? images  // These are now full URLs from upload API
          : (analysis.useRecentImage && conversationContext?.recentImageUrl) 
            ? [conversationContext.recentImageUrl] 
            : []
            
        if (imagesToUse.length === 0) {
          response = 'I need images to edit or modify. Please upload some images or generate an image first that I can reference.'
        } else {
          try {
            console.log('Sending images to Wavespeed:', imagesToUse) // Debug log
            console.log('Image URLs being sent:', imagesToUse.map(url => ({ url, accessible: 'checking...' }))) // Debug log
            
            // Check and deduct credits
            const imageEditCredits = await checkAndDeductCreditsForGeneration(
              session.user.id,
              'IMAGE_TO_IMAGE'
            )
            
            if (!imageEditCredits.success) {
              response = `Sorry, you don't have enough credits to edit images. You need ${imageEditCredits.cost} credits. Please upgrade your plan to continue.`
              break
            }
            
            console.log(`ðŸ’³ Credits deducted: ${imageEditCredits.cost}`)
            
            const requestId = await WavespeedService.imageToImage(
              analysis.prompt,
              imagesToUse,
              userPreferences.imageToImageModel,
              userPreferences.aspectRatio
            )
            
            const generation = await prisma.generation.create({
              data: {
                messageId: userMessage.id,
                type: 'IMAGE_TO_IMAGE',
                status: 'PROCESSING',
                prompt: analysis.prompt,
                provider: 'wavespeed',
                model: `${userPreferences.imageToImageModel.toLowerCase()}`,
                requestId,
                metadata: { creditsUsed: imageEditCredits.cost }
              },
            })
            
            generations.push(generation)
            
            const imageSource = images.length > 0 ? 'uploaded images' : 'previous image'
            response = `I'm editing your ${imageSource} based on: "${analysis.prompt}". This will take a few moments... (${imageEditCredits.cost} credits used)`
          } catch (error: any) {
            console.error('Image-to-image generation error:', error)
            console.error('Error details:', error.response?.data) // More detailed error logging
            let errorMessage = 'Sorry, I encountered an error while trying to edit your images.'
            
            // Extract more specific error information
            if (error.response?.data?.message) {
              errorMessage = `Error: ${error.response.data.message}`
            } else if (error.message?.includes('invalid url')) {
              errorMessage = 'Error: The image format is not supported. Please try uploading a different image.'
            } else if (error.message) {
              errorMessage = `Error: ${error.message}`
            }
            
            response = errorMessage + ' Please try again with a different image or prompt.'
          }
        }
        break

      case 'text_to_video':
        // Generate an image first, then offer to create video from it
        try {
          console.log('ðŸŽ¨ Generating image first for video request:', analysis.prompt)
          
          // Generate image using the user's video prompt
          const imageTaskId = await WavespeedService.textToImage(
            analysis.prompt,
            userPreferences?.textToImageModel || 'SEEDREAM_V4',
            userPreferences?.aspectRatio || 'WIDE'
          )
          
          // Create generation record for the image
          const imageGeneration = await prisma.generation.create({
            data: {
              messageId: userMessage.id,
              type: 'TEXT_TO_IMAGE',
              status: 'PROCESSING',
              prompt: analysis.prompt,
              provider: 'wavespeed',
              model: userPreferences?.textToImageModel || 'SEEDREAM_V4',
              requestId: imageTaskId,
            },
          })
          
          generations.push(imageGeneration)
          
          response = `For better video results, I will be creating an image first from your request: "${analysis.prompt}". Once the image is ready, you'll be able to generate a video from it.`
        } catch (error: any) {
          console.error('Error generating image for video request:', error)
          response = 'Sorry, I encountered an error while trying to create an image from your video request. Please try again.'
        }
        break

      case 'video_duration_selection':
        // Check user's video model preference to determine available durations
        const videoModel = userPreferences?.videoModel || 'WAN_2_5'
        if (videoModel === 'VEO3_FAST') {
          response = `I'd be happy to create a video with your prompt: "${analysis.prompt}"\n\nPlease choose the duration:\n\n**5 Seconds** | **8 Seconds**\n\nNote: Google VEO3-Fast supports 5-8 second videos.`
        } else {
          // Default to WAN-2.5 options
          response = `I'd be happy to create a video with your prompt: "${analysis.prompt}"\n\nPlease choose the duration:\n\n**5 Seconds** | **10 Seconds**\n\nNote: Alibaba WAN-2.5 supports 5-10 second videos.`
        }
        break

      case 'image_to_video':
        // Handle both uploaded images and recent image references
        const videoImagesToUse = images.length > 0 
          ? images 
          : (analysis.useRecentImage && conversationContext?.recentImageUrl) 
            ? [conversationContext.recentImageUrl] 
            : []
            
        if (videoImagesToUse.length === 0) {
          response = 'I need images to create a video. Please upload some images or generate an image first that I can reference.'
        } else {
          try {
            // Extract duration from prompt if specified
            const durationMatch = analysis.prompt.match(/(\d+)\s*(seconds?|s\b)/i)
            const requestedDuration = durationMatch ? parseInt(durationMatch[1]) : 5
            
            // Determine model based on user preferences
            const videoModel = userPreferences?.videoModel || 'WAN_2_5'
            
            // Validate and map duration to supported values
            const durationValidation = validateAndMapVideoDuration(requestedDuration, videoModel)
            const finalDuration = durationValidation.duration
            const durationMessage = durationValidation.message
            
            const taskId = await WanService.generateVideo(
              analysis.prompt,
              videoImagesToUse[0],
              finalDuration,
              '720p',
              videoModel
            )
            
            const generation = await prisma.generation.create({
              data: {
                messageId: userMessage.id,
                type: 'IMAGE_TO_VIDEO',
                status: 'PROCESSING',
                prompt: analysis.prompt,
                provider: 'wavespeed',
                model: videoModel === 'VEO3_FAST' ? 'veo3-fast' : 'wan-2.5',
                requestId: taskId,
              },
            })
            
            generations.push(generation)
            
            const imageSource = images.length > 0 ? 'your images' : 'the previous image'
            const baseResponse = `I'm creating a ${finalDuration}-second video from ${imageSource} based on: "${analysis.prompt}". This will take a few minutes...`
            
            // Add duration adjustment message if needed
            response = durationMessage ? `${durationMessage}\n\n${baseResponse}` : baseResponse
          } catch (error) {
            response = 'Sorry, I encountered an error while trying to create your video. Please try again.'
          }
        }
        break

      case 'lipsync':
        // Handle lipsync generation - requires both image and audio
        const lipsyncImageToUse = images.length > 0 
          ? images[0]  // Use first uploaded image
          : (analysis.useRecentImage && conversationContext?.recentImageUrl) 
            ? conversationContext.recentImageUrl 
            : null
            
        const lipsyncAudioToUse = audio.length > 0 ? audio[0] : null // Use first uploaded audio
            
        if (!lipsyncImageToUse || !lipsyncAudioToUse) {
          if (!lipsyncImageToUse && !lipsyncAudioToUse) {
            response = 'To create a lipsync video, I need both an image and an audio file. Please upload an image and an audio file.'
          } else if (!lipsyncImageToUse) {
            response = 'To create a lipsync video, I need an image. Please upload an image along with your audio file.'
          } else {
            response = 'To create a lipsync video, I need an audio file. Please upload an audio file with your speech or voice.'
          }
        } else {
          try {
            console.log('Generating lipsync with:', { image: lipsyncImageToUse, audio: lipsyncAudioToUse, prompt: analysis.prompt }) // Debug log
            
            // Estimate lipsync duration (assume 10 seconds for now, can be refined with actual audio duration)
            const estimatedDuration = 10
            
            // Check and deduct credits
            const lipsyncCredits = await checkAndDeductCreditsForGeneration(
              session.user.id,
              'LIPSYNC',
              estimatedDuration
            )
            
            if (!lipsyncCredits.success) {
              response = `Sorry, you don't have enough credits to generate a lipsync video. You need ${lipsyncCredits.cost} credits. Please upgrade your plan to continue.`
              break
            }
            
            console.log(`ðŸ’³ Credits deducted for lipsync: ${lipsyncCredits.cost}`)
            
            const requestId = await WavespeedService.lipsync(
              lipsyncAudioToUse,
              lipsyncImageToUse,
              analysis.prompt
            )
            
            const generation = await prisma.generation.create({
              data: {
                messageId: userMessage.id,
                type: 'LIPSYNC',
                status: 'PROCESSING',
                prompt: analysis.prompt,
                provider: 'wavespeed',
                model: 'infinitetalk',
                requestId,
                metadata: { creditsUsed: lipsyncCredits.cost }
              },
            })
            
            generations.push(generation)
            
            const imageSource = images.length > 0 ? 'your image' : 'the previous image'
            response = `I'm creating a lipsync video using ${imageSource} and your audio. This will take a few moments... (${lipsyncCredits.cost} credits used)`
          } catch (error: any) {
            console.error('Lipsync generation error:', error)
            console.error('Error details:', error.response?.data) // More detailed error logging
            response = 'Sorry, I encountered an error while trying to create your lipsync video. Please try again.'
          }
        }
        break

      case 'text_to_audio':
        try {
          // Extract text from quotes in the ORIGINAL MESSAGE (not analysis.prompt)
          const textMatch = message.match(/"([^"]*)"/) || message.match(/'([^']*)'/)
          const textToSpeak = textMatch ? textMatch[1] : analysis.prompt
          
          // Try to match voice description with AI if user provided voice characteristics
          voiceId = 'English_compelling_lady1' // Default fallback
          
          // Check if user provided voice description in the ORIGINAL MESSAGE
          const hasVoiceDescription = message.toLowerCase().includes('female') || 
                                    message.toLowerCase().includes('male') || 
                                    message.toLowerCase().includes('voice') || 
                                    message.toLowerCase().includes('accent') ||
                                    message.toLowerCase().includes('adult') || 
                                    message.toLowerCase().includes('young') ||
                                    message.toLowerCase().includes('old') ||
                                    message.toLowerCase().includes('deep') || 
                                    message.toLowerCase().includes('soft') ||
                                    message.toLowerCase().includes('loud') || 
                                    message.toLowerCase().includes('gentle') ||
                                    message.toLowerCase().includes('strong') || 
                                    message.toLowerCase().includes('smooth')
          
          console.log('ðŸŽ¤ Voice description check:', {
            originalMessage: message,
            extractedPrompt: analysis.prompt,
            hasVoiceDescription,
            containsMale: message.toLowerCase().includes('male'),
            containsOld: message.toLowerCase().includes('old')
          })
          
          if (hasVoiceDescription) {
            try {
              // Extract gender from the ORIGINAL MESSAGE
              const isFemale = message.toLowerCase().includes('female')
              const isMale = message.toLowerCase().includes('male')
              const gender = isFemale ? 'Female' : isMale ? 'Male' : 'Female' // Default to female
              
          console.log('ðŸŽ¤ Voice matching - Gender:', gender, 'Description:', message)
          
          // Use AI to match voice using the ORIGINAL MESSAGE (contains voice description)
          const { matchVoiceWithAI } = await import('@/lib/voice-utils')
          voiceId = await matchVoiceWithAI(message, gender, 'English')
          
          console.log('ðŸŽ¤ âœ… FINAL SELECTED VOICE ID:', voiceId)
          console.log('ðŸŽ¤ âœ… Voice ID being sent to Wavespeed API:', voiceId)
            } catch (error) {
              console.error('Voice matching error:', error)
              
              // Better fallback based on gender from ORIGINAL MESSAGE
              if (message.toLowerCase().includes('male')) {
                voiceId = 'English_magnetic_voiced_man' // Male fallback
                console.log('ðŸŽ¤ Using male fallback voice:', voiceId)
              } else {
                voiceId = 'English_compelling_lady1' // Female fallback
                console.log('ðŸŽ¤ Using female fallback voice:', voiceId)
              }
            }
          }
          
          console.log('ðŸŽ¤ Generating audio with text:', textToSpeak) // Debug log
          console.log('ðŸŽ¤ Final voice ID being used:', voiceId) // Debug log
          
          // Generate audio using Wavespeed API directly
          const requestBody = {
            emotion: 'happy',
            enable_sync_mode: false,
            english_normalization: false,
            pitch: 0,
            speed: 1,
            text: textToSpeak,
            voice_id: voiceId,
            volume: 1
          }
          
          console.log('ðŸŽ¤ Wavespeed API request body:', JSON.stringify(requestBody, null, 2))
          
          const wavespeedResponse = await fetch('https://api.wavespeed.ai/api/v3/minimax/speech-02-hd', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${process.env.WAVESPEED_API_KEY}`
            },
            body: JSON.stringify(requestBody)
          })

          if (!wavespeedResponse.ok) {
            const errorText = await wavespeedResponse.text()
            console.error('Wavespeed API error:', errorText)
            throw new Error(`Wavespeed API error: ${errorText}`)
          }

          const wavespeedData = await wavespeedResponse.json()
          console.log('Wavespeed response:', wavespeedData)

          // Extract the request ID from the response
          const requestId = wavespeedData.data?.id || wavespeedData.id
          console.log('Request ID:', requestId)

          // Poll for completion
          let attempts = 0
          const maxAttempts = 120 // 10 minutes max (120 * 5 seconds = 600 seconds = 10 minutes)
          let audioUrl = null

          while (attempts < maxAttempts && !audioUrl) {
            await new Promise(resolve => setTimeout(resolve, 5000)) // Wait 5 seconds
            attempts++

            try {
              console.log(`Status check attempt ${attempts}/${maxAttempts}...`)
              const statusResponse = await fetch(`https://api.wavespeed.ai/api/v3/predictions/${requestId}/result`, {
                headers: {
                  'Authorization': `Bearer ${process.env.WAVESPEED_API_KEY}`
                }
              })

              if (statusResponse.ok) {
                const statusData = await statusResponse.json()
                console.log('Status check response:', statusData)

                // Extract status and outputs from the response (could be nested in data or at root level)
                const status = statusData.data?.status || statusData.status
                const outputs = statusData.data?.outputs || statusData.outputs
                const error = statusData.data?.error || statusData.error

                if (status === 'completed' && outputs && outputs.length > 0) {
                  audioUrl = outputs[0]
                  console.log('Audio generation completed! URL:', audioUrl)
                  break
                } else if (status === 'failed') {
                  console.error('Audio generation failed:', error)
                  throw new Error('Audio generation failed')
                }
              } else {
                console.error('Status check failed:', statusResponse.status, statusResponse.statusText)
              }
            } catch (error) {
              console.error('Status check error:', error)
              if (attempts >= maxAttempts) {
                throw error
              }
            }
          }

          if (!audioUrl) {
            throw new Error('Audio generation timed out')
          }

          // Download and save the audio file
          const audioResponse = await fetch(audioUrl)
          if (!audioResponse.ok) {
            throw new Error('Failed to download audio')
          }

          const audioBuffer = await audioResponse.arrayBuffer()
          const fileName = `audio_${Date.now()}.mp3`
          const { saveFile } = await import('@/lib/media-storage')
          const localPath = await saveFile(audioBuffer, fileName, 'audio')

          // Create generation record with COMPLETED status since we already have the result
          const generation = await prisma.generation.create({
            data: {
              messageId: userMessage.id,
              type: 'TEXT_TO_AUDIO' as any,
              status: 'COMPLETED',
              prompt: textToSpeak,
              provider: 'wavespeed',
              model: 'minimax/speech-02-hd',
              requestId: requestId,
              resultUrl: localPath,
              resultUrls: [localPath],
            },
          })

          // Save audio to user's collection
          await prisma.savedAudio.create({
            data: {
              userId: session.user.id,
              title: `Audio: ${textToSpeak.substring(0, 50)}...`,
              prompt: textToSpeak,
              originalUrl: audioUrl,
              localPath: localPath,
              fileName: fileName,
              fileSize: audioBuffer.byteLength,
              voiceId: voiceId,
              language: 'English',
              mimeType: 'audio/mpeg',
              generationId: generation.id
            }
          })
          
          generations.push(generation)
          response = `I've created an audio file with the text: "${textToSpeak}". The audio is ready to play!`
        } catch (error: any) {
          console.error('Text-to-audio generation error:', error)
          response = 'Sorry, I encountered an error while trying to create your audio. Please try again.'
        }
        break

      case 'chat':
      default:
        // Use the specific prompt from AI orchestrator if it's a detailed response
        if (analysis.prompt && analysis.prompt.length > 100) {
          response = analysis.prompt
        } else {
          response = `I understand you want to ${analysis.prompt}. I'm an AI assistant specialized in creating and editing images, videos, and audio. Here's what I can help you with:

ðŸ–¼ï¸ **Create Images**: Describe any image you want me to generate
ðŸŽ¨ **Edit Images**: Upload images and tell me how to modify them
ðŸŽ¥ **Create Videos**: Describe a video or upload images to animate
ðŸŽµ **Create Audio**: Ask me to create audio with text in quotes like "Hello world"
ðŸ’¬ **Ask Questions**: I'm here to help with your creative projects!

What would you like to create today?`
        }
        break
    }

    // Create assistant message first
    const assistantMessage = await prisma.message.create({
      data: {
        chatId: chat.id,
        role: 'ASSISTANT',
        content: response,
      },
    })

    // Move generations from user message to assistant message if any were created
    if (generations.length > 0) {
      await prisma.generation.updateMany({
        where: {
          messageId: userMessage.id,
        },
        data: {
          messageId: assistantMessage.id,
        },
      })
    }

    // Get the updated assistant message with generations
    const updatedAssistantMessage = await prisma.message.findUnique({
      where: { id: assistantMessage.id },
      include: { generations: true },
    })

    return NextResponse.json({
      response,
      chatId: chat.id,
      messageId: assistantMessage.id,
      userMessageId: userMessage.id,
      analysis,
      generations: updatedAssistantMessage?.generations || [],
      debugInfo: {
        selectedVoiceId: analysis.action === 'text_to_audio' ? voiceId : null
      }
    })

  } catch (error: any) {
    console.error('ðŸ’¥ === CHAT API ERROR ===') // Debug log
    console.error('ðŸ’¥ Error type:', error?.constructor?.name) // Debug log
    console.error('ðŸ’¥ Error message:', error?.message) // Debug log
    console.error('ðŸ’¥ Full error:', error) // Debug log
    console.error('ðŸ’¥ Stack trace:', error?.stack) // Debug log
    
    if (error instanceof z.ZodError) {
      console.error('ðŸ’¥ Zod validation error:', error.errors) // Debug log
      return NextResponse.json(
        { error: 'Invalid input', details: error.errors },
        { status: 400 }
      )
    }

    console.error('ðŸ’¥ Returning 500 error to client') // Debug log
    return NextResponse.json(
      { error: 'Internal server error', details: error?.message },
      { status: 500 }
    )
  }
}